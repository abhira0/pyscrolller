{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "laughing-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import json\n",
    "from praw.models.listing.mixins import submission\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import threading\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryWait(function_name, argz: list, timeout: int):\n",
    "    \"\"\"Tries for given time and returns the return_object on success\"\"\"\n",
    "    for i in range(timeout):\n",
    "        try:\n",
    "            return_objs = function_name(*argz)\n",
    "            if return_objs:\n",
    "                return return_objs\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "    print(f\"[!] Function execution failed @ {function_name}\")\n",
    "\n",
    "def tryExcept(function_name, argz:list, timeout: int):\n",
    "    \"\"\"Tries for given time and returns tries_left on success\"\"\"\n",
    "    for i in range(timeout):\n",
    "        try:\n",
    "            function_name(*argz)\n",
    "            return timeout-i-1\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "    print(f\"[!] Function execution failed @ {function_name}\")\n",
    "\n",
    "\n",
    "def over_18_button():\n",
    "    driver.find_element_by_css_selector(\"div.Wall-Button\").click()\n",
    "def getDriver(url):\n",
    "    gecko_path = r\"E:\\Downloads\\IDM\\Compressed\\geckodriver.exe\"\n",
    "    opts = Options()\n",
    "    # opts.set_preference(\"permissions.default.image\", 2)\n",
    "    # opts.set_preference(\"dom.ipc.plugins.enabled.libflashplayer.so\", \"false\")\n",
    "    driver = webdriver.Firefox(executable_path=gecko_path, options=opts)\n",
    "    driver.get(url)\n",
    "    # driver.set_window_size(1000,1000)\n",
    "    return driver\n",
    "def skip_warning(driver):\n",
    "    driver.find_element_by_css_selector(\".nsfw-warning__accept-button\").click()\n",
    "\n",
    "def replace_chars(name: str, keywords: str, to: str):\n",
    "    for i in keywords:\n",
    "        name = name.replace(i, to)\n",
    "    return name\n",
    "\n",
    "def getTitleURL(driver):\n",
    "    title= driver.find_element_by_css_selector(\"div.fullscreen-view__entry-container > div > img\").get_attribute(\"src\")\n",
    "    replace_chars(title, '<>:\"/\\|?*.', \"\")\n",
    "    url = driver.find_element_by_css_selector(\"div.fullscreen-view__entry-container:nth-child(1) > div:nth-child(1) > div:nth-child(2) > div:nth-child(1) > div:nth-child(1) > div:nth-child(2)\").get_attribute(\"innerHTML\").strip()\n",
    "    return title, url\n",
    "\n",
    "def scrapeScrollerSite(driver, content_type, ultimatum):\n",
    "    flag = 100\n",
    "    driver = getDriver(url+\"?filter=\"+content_type)\n",
    "    tryExcept(skip_warning, [driver],10)\n",
    "    time.sleep(3)\n",
    "    while True:\n",
    "        if flag<0:\n",
    "            print(\"GONNA BREAK!\")\n",
    "            break\n",
    "        try:\n",
    "            a_tags = driver.find_elements_by_css_selector(\"div.vertical-view__column > div > a\")\n",
    "            for a_tag in a_tags:\n",
    "                href = a_tag.get_attribute(\"href\")\n",
    "                if href in ultimatum:\n",
    "                    flag-=1\n",
    "                    print(f\"Links[{len(ultimatum)}]\", end=\"\\r\")\n",
    "                else:\n",
    "                    flag = 100\n",
    "                    ultimatum[href] = {\"type\":content_type, \"title\": \"\", \"media_links\":[]}\n",
    "                    print(f\"Links[{len(ultimatum)}]: {href}\", end=\"\\r\")\n",
    "            scroll(driver)\n",
    "            # time.sleep(0.1)\n",
    "        except:\n",
    "            scroll(driver)\n",
    "            time.sleep(0.1)\n",
    "    driver.close()\n",
    "\n",
    "def SourceCode(url, selector):\n",
    "    source_Code = requests.get(url)\n",
    "    # for i in re.findall(r'https?://[^\"\\\\]+', str(source_Code.content)):\n",
    "    #     print(i)\n",
    "    plain_Text = source_Code.text\n",
    "    soup = BeautifulSoup(plain_Text, \"html.parser\")\n",
    "    print(soup)\n",
    "    soup_Select = soup.select(selector)\n",
    "    return soup_Select\n",
    "def fun(url):\n",
    "    x = requests.get(url)\n",
    "    for i in re.findall(r'https?://[^\"\\\\]+', str(x.content)):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ultimatum = {}\n",
    "url = \"https://scrolller.com/indianbabes\"\n",
    "content_types = [\"pictures\", \"videos\", \"albums\"][:-2]\n",
    "for content_type in content_types:\n",
    "    thread_list = []\n",
    "    for i in range(2):\n",
    "        p = threading.Thread(target=scrapeScrollerSite, args=(driver, content_type,ultimatum,))\n",
    "        thread_list.append(p)\n",
    "        p.start()\n",
    "    for i in thread_list:\n",
    "        p.join()\n",
    "len(ultimatum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActionChains(driver).move_to_element()\n",
    "def scroll(driver, last_height, add_height):\n",
    "    new_height = last_height+add_height\n",
    "    driver.execute_script(f\"window.scrollTo(0, {new_height});\")    # Scroll down to bottom\n",
    "    return new_height\n",
    "def getLinksFromWindow():\n",
    "    cols = driver.find_elements_by_css_selector(\".vertical-view__column\")\n",
    "    for col in cols:\n",
    "        items = col.find_elements_by_css_selector(\".vertical-view__item\")\n",
    "        for item in items:\n",
    "            try:\n",
    "                ActionChains(driver).move_to_element(item).perform()\n",
    "                time.sleep(0.1)\n",
    "                source_tags = item.find_elements_by_css_selector(\"video source\")\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            if source_tags:\n",
    "                title = item.find_element_by_css_selector(\"div.item-panel__description\").get_attribute(\"innerHTML\")\n",
    "                url = item.find_element_by_css_selector(\"a\").get_attribute(\"href\")\n",
    "                src = source_tags[-1].get_attribute(\"src\")\n",
    "                print(title)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from praw.models.listing.mixins import submission\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from termcolor import cprint\n",
    "from utils import *\n",
    "\n",
    "class Downloader:\n",
    "    def over_18_button(self, driver):\n",
    "        driver.find_element_by_css_selector(\".nsfw-warning__accept-button\").click()\n",
    "\n",
    "    def welcome_pop(self, driver):\n",
    "        driver.find_element_by_css_selector(\".notification__title\").click()\n",
    "\n",
    "    def scroll(self, driver, last_height, add_height):\n",
    "        new_height = last_height + add_height\n",
    "        driver.execute_script(\n",
    "            f\"window.scrollTo(0, {new_height});\"\n",
    "        )  # Scroll down to bottom\n",
    "        return new_height\n",
    "\n",
    "    def cssFindAttr(self, item, selector: str, attribute: str) -> str:\n",
    "        temp = item.find_element_by_css_selector(selector).get_attribute(attribute)\n",
    "        return temp\n",
    "\n",
    "    def cssFind(self, item, selector: str):\n",
    "        temp = item.find_elements_by_css_selector(selector)\n",
    "        return temp\n",
    "\n",
    "    def moveTo(self, driver, element):\n",
    "        ActionChains(driver).move_to_element(element).perform()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def ultimatumPut(self, ultimatum, url, typ, title, media_links):\n",
    "        ultimatum.update(\n",
    "            {url: {\"type\": typ, \"title\": title, \"media_links\": media_links}}\n",
    "        )\n",
    "\n",
    "    def getContentCount(self, typ):\n",
    "        count = 0\n",
    "        for i, j in self.ultimatum.items():\n",
    "            if j[\"type\"] == typ:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def begin(self):\n",
    "        self.num_of_ele = self.getContentCount(self.typ)\n",
    "        scrape_url = f\"https://scrolller.com/r/{self.sub_name}?filter={self.typ}\"\n",
    "        state = State(f\"{str(self.typ).upper()}: {scrape_url}\", False)\n",
    "        state.openURL(scrape_url)\n",
    "        utils.tryExcept(self.welcome_pop, [state.driver], 10)\n",
    "        utils.tryExcept(self.over_18_button, [state.driver], 10)\n",
    "        time.sleep(3)\n",
    "        self.driver = state.driver\n",
    "        # self.scrapeAll()\n",
    "        # self.driver.close()\n",
    "\n",
    "class Picture(Downloader):\n",
    "    typ = \"pictures\"\n",
    "\n",
    "    def __init__(self, sub_name: str, ultimatum: dict) -> None:\n",
    "        \"\"\"A class dedicated to scrape pictures links by applying pictures filter to the url\n",
    "\n",
    "        Args:\n",
    "            sub_name (str): Name of the subreddit\n",
    "            ultimatum (dict): The ultimate dictionary used to save the scraped data.\n",
    "        \"\"\"\n",
    "        self.ultimatum = ultimatum\n",
    "        self.sub_name = sub_name\n",
    "\n",
    "    def scrapeAll(self):\n",
    "        screen_width = self.driver.execute_script(\"return screen.width\")\n",
    "        screen_height = self.driver.execute_script(\"return screen.height\")\n",
    "        d_screen_width = screen_width\n",
    "        d_screen_height = screen_height * 3\n",
    "        self.driver.set_window_size(d_screen_width, d_screen_height)\n",
    "        height = self.scroll(self.driver, 0, 0)\n",
    "        stop_flag = 500 if self.num_of_ele + 1 < 500 else self.num_of_ele + 1\n",
    "        # print(stop_flag)\n",
    "        while stop_flag > 0:\n",
    "            ret = self.getLinksFromWindow()\n",
    "            if ret == 0:\n",
    "                stop_flag = 500 if self.num_of_ele + 1 < 500 else self.num_of_ele + 1\n",
    "            else:\n",
    "                stop_flag -= ret\n",
    "            height = self.scroll(self.driver, height, screen_height * 2)\n",
    "\n",
    "    def getMetadata(self, img_item) -> tuple:\n",
    "        src = self.cssFindAttr(img_item, \"img\", \"srcset\").split(\",\")[-1].split(\" \")[0]\n",
    "        while src!=\"\":\n",
    "            src = self.cssFindAttr(img_item, \"img\", \"srcset\").split(\",\")[-1].split(\" \")[0]\n",
    "            print(src)\n",
    "        title = self.cssFindAttr(img_item, \".item-panel__description\", \"innerHTML\")\n",
    "        while title!=\"\":\n",
    "            title = self.cssFindAttr(img_item, \".item-panel__description\", \"innerHTML\")\n",
    "            print(title)\n",
    "        return title, src\n",
    "\n",
    "    def getLinksFromWindow(self):\n",
    "        c_already_present = 0\n",
    "        c_added = 0\n",
    "        for img_item in self.cssFind(self.driver, \".vertical-view__item\"):\n",
    "            try:\n",
    "                url = self.cssFindAttr(img_item, \"a[rel=nofollow]\", \"href\")\n",
    "                img_item = WebDriverWait(self.driver, 10).until(lambda x: img_item)\n",
    "                if url not in self.ultimatum.keys():\n",
    "                    self.moveTo(self.driver, img_item)\n",
    "                    title, src = self.getMetadata(img_item)\n",
    "                    self.ultimatumPut(self.ultimatum, url, \"pictures\", title, [src])\n",
    "                    c_added += 1\n",
    "                    self.num_of_ele += 1\n",
    "                else:\n",
    "                    c_already_present += 1\n",
    "                print(f\"ULTIMATUM [{len(self.ultimatum)}]\", end=\"\\r\")\n",
    "            except Exception as e:\n",
    "                # print(url,e)\n",
    "                ...\n",
    "        # return 0 if at least one link is added to ultimatum\n",
    "        return c_already_present if c_added == 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36m\t[i] Instantiating a headless Firefox: 'PICTURES: https://scrolller.com/r/nippleplay?filter=pictures'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ins = Picture(\"nippleplay\", {})\n",
    "ins.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.scrapeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}